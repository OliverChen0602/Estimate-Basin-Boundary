{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve on the deep learning appraoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code to reload the model\n",
    "net = Net()\n",
    "\n",
    "# Load the model\n",
    "net.load_state_dict(torch.load('lorenz_model_win.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New data generation and additional penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose an adaptive approach to enhance our model with new data: After a training phase, we consider the datapoints used as an estimation of the decision boundary, which are the points with probability predicted close to 0.5. We create new datapoints by adding small perturbations to such points with the following scheme:\n",
    "1. For such a datapoint $x=(x_{0},y_{0},z_{0})$, we create $y=(x_{0}+\\delta_{0},y_{0}+\\delta_{1},z_{0}+\\delta_{2})$, where $\\delta_{i} \\sim \\mathbb{N}(0, \\sigma^{2})$ is a Gaussian noise, and the variance $\\sigma^{2}$ is positively related with the probability of $x$.\n",
    "2. We add the created datapoints to the dataset and train again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we expect that at the basin boundary, the direction of the dynamical system should be perpendicular to the gradient vector of the level set $f(x)=0$, we add an penalty for violation.\n",
    "\n",
    "Let the direction of the dynamical system at point $\\mathbf{x}$ be $b(\\mathbf{x})$, then we expect that $p = \\nabla f(\\mathbf{x})^{T}b(\\mathbf{x})=0$. We add penalty for violation:\n",
    "\n",
    "1. $\\log{p}$ (logarithm penalty)\n",
    "2. $p^{k}$ (polynomial penalty, we try linear, quadratic and cubic)\n",
    "3. $e^{p}$ (exponential penalty)\n",
    "\n",
    "Remark: When we compute $p$, both vectors are normalized as we are interested with the angular violation. If we do not normalize both vectors, $p$ may be large even if two vectors are almost perpendicular, as long as one of them has a large norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_perturbation(data, probability):\n",
    "    diff = abs(probability - 0.5)\n",
    "    k = 2 * diff ## factor 2 is a hyperparameter\n",
    "    noise = torch.randn_like(data) * torch.sqrt(torch.tensor(k))\n",
    "    perturbed_data = data + noise\n",
    "    return perturbed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLossLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, net, x, y, v):\n",
    "        # Compute the output and the BCE loss\n",
    "        out = net(x)\n",
    "        bce_loss = self.bce_loss(out, y)\n",
    "\n",
    "        # Compute the gradient of the output with respect to the input\n",
    "        out.backward(torch.ones_like(out), retain_graph=True)\n",
    "        grad = x.grad\n",
    "        # Normalize the gradient\n",
    "        grad_norm = grad / grad.norm()\n",
    "\n",
    "        # Compute the inner product of the normalized gradient and v\n",
    "        inner_product = torch.abs(torch.dot(grad_norm.view(-1), v.view(-1)))\n",
    "\n",
    "        # The final loss is the sum of the BCE loss and the inner product\n",
    "        loss = bce_loss + inner_product\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class CustomLossQuadratic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, net, x, y, v):\n",
    "        # Compute the output and the BCE loss\n",
    "        out = net(x)\n",
    "        bce_loss = self.bce_loss(out, y)\n",
    "\n",
    "        # Compute the gradient of the output with respect to the input\n",
    "        out.backward(torch.ones_like(out), retain_graph=True)\n",
    "        grad = x.grad\n",
    "        # Normalize the gradient\n",
    "        grad_norm = grad / grad.norm()\n",
    "\n",
    "        # Compute the inner product of the normalized gradient and v\n",
    "        inner_product = torch.abs(torch.dot(grad_norm.view(-1), v.view(-1)))\n",
    "\n",
    "        # The final loss is the sum of the BCE loss and the inner product\n",
    "        loss = bce_loss + inner_product**2\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class CustomLossLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, net, x, y, v):\n",
    "        # Compute the output and the BCE loss\n",
    "        out = net(x)\n",
    "        bce_loss = self.bce_loss(out, y)\n",
    "\n",
    "        # Compute the gradient of the output with respect to the input\n",
    "        out.backward(torch.ones_like(out), retain_graph=True)\n",
    "        grad = x.grad\n",
    "        # Normalize the gradient\n",
    "        grad_norm = grad / grad.norm()\n",
    "\n",
    "        # Compute the inner product of the normalized gradient and v\n",
    "        inner_product = torch.abs(torch.dot(grad_norm.view(-1), v.view(-1)))\n",
    "\n",
    "        # The final loss is the sum of the BCE loss and the inner product\n",
    "        loss = bce_loss + inner_product\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class CustomLossLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, net, x, y, v):\n",
    "        # Compute the output and the BCE loss\n",
    "        out = net(x)\n",
    "        bce_loss = self.bce_loss(out, y)\n",
    "\n",
    "        # Compute the gradient of the output with respect to the input\n",
    "        out.backward(torch.ones_like(out), retain_graph=True)\n",
    "        grad = x.grad\n",
    "        # Normalize the gradient\n",
    "        grad_norm = grad / grad.norm()\n",
    "\n",
    "        # Compute the inner product of the normalized gradient and v\n",
    "        inner_product = torch.abs(torch.dot(grad_norm.view(-1), v.view(-1)))\n",
    "\n",
    "        # The final loss is the sum of the BCE loss and the inner product\n",
    "        loss = bce_loss + inner_product\n",
    "\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
