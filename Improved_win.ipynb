{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve on the deep learning appraoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from scipy.integrate import solve_ivp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the Lorenz system\n",
    "sigma = 10\n",
    "beta = 8/3\n",
    "r = 10\n",
    "\n",
    "def lorenz(t, X, sigma, beta, r):\n",
    "    \"\"\"The Lorenz equations.\"\"\"\n",
    "    x, y, z = X\n",
    "    xp = sigma*(y - x)\n",
    "    yp = r*x - y - x*z\n",
    "    zp = -beta*z + x*y\n",
    "    return xp, yp, zp\n",
    "\n",
    "def lorenz_data(u):\n",
    "    x, y, z = u[0], u[1], u[2]\n",
    "    return [sigma*(y - x), r*x - y - x*z, -beta*z + x*y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the trajectory is attracted to the concerned Lorenz attractor\n",
    "def is_attracted(x, y, z):\n",
    "    return (abs(x-math.sqrt(24))<0.01) and (abs(y-math.sqrt(24))<0.01) and (abs(z-9)<0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement the simulation process and decide if the trajectory is attracted by the Lorenz attractor\n",
    "def simulation(x0 ,y0 ,z0):\n",
    "    tmax, n = 1500, 100000\n",
    "    soln = solve_ivp(lorenz, (0, tmax), (x0, y0, z0), args=(sigma, beta, r),dense_output=True)\n",
    "    t = np.linspace(0, tmax, n)\n",
    "    x, y, z = soln.sol(t)\n",
    "    return is_attracted(x[n-1], y[n-1], z[n-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the dataset\n",
    "class LorenzDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor([self.data.iloc[idx].x0, self.data.iloc[idx].y0, self.data.iloc[idx].z0]).float()\n",
    "        label = torch.tensor(self.data.iloc[idx].attracted).float()\n",
    "        return features, label\n",
    "    \n",
    "dataset = LorenzDataset('dataset_large.csv')\n",
    "\n",
    "dataset_train, dataset_test = torch.utils.data.random_split(dataset, [80000, 20000])\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New data generation and additional penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose an adaptive approach to enhance our model with new data: After a training phase, we consider the datapoints used as an estimation of the decision boundary, which are the points with probability predicted close to 0.5. We create new datapoints by adding small perturbations to such points with the following scheme:\n",
    "1. For such a datapoint $x=(x_{0},y_{0},z_{0})$, we create $y=(x_{0}+\\delta_{0},y_{0}+\\delta_{1},z_{0}+\\delta_{2})$, where $\\delta_{i} \\sim \\mathbb{N}(0, \\sigma^{2})$ is a Gaussian noise, and the variance $\\sigma^{2}$ is positively related with the probability of $x$.\n",
    "2. We add the created datapoints to the dataset and train again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we expect that at the basin boundary, the direction of the dynamical system should be perpendicular to the gradient vector of the level set $f(x)=0$, we add an penalty for violation.\n",
    "\n",
    "Let the direction of the dynamical system at point $\\mathbf{x}$ be $b(\\mathbf{x})$, then we expect that $p = \\nabla f(\\mathbf{x})^{T}b(\\mathbf{x})=0$. We add penalty for violation:\n",
    "\n",
    "1. $p$ (linear penalty)\n",
    "2. $e^{p}$ (exponential penalty)\n",
    "3. $\\tan{\\frac{\\pi}{2}p}$ (tangent penalty)\n",
    "\n",
    "Remark: When we compute $p$, both vectors are normalized as we are interested with the angular violation. If we do not normalize both vectors, $p$ may be large even if two vectors are almost perpendicular, as long as one of them has a large norm.\n",
    "\n",
    "Furthermore, if a point is far away from the estimated boundary(probability far away from 0.5), then it should not contribute to penalty as the normality condition is mainly for points close to the boundary. Therefore, the penalty term is formulated as follows:\n",
    "$$\\sum_{\\mathbf{x}}l(\\mathbf{x})+c(\\mathbf{x})m(p)$$\n",
    "\n",
    "where $l(\\mathbf{x})$ is the logistic loss, $m(p)=p,e^{p},\\tan{\\frac{\\pi}{2}p}$, and $c(\\mathbf{x})$ is an indicator function to only filter out those points that the predicted values are within $[0.5-\\delta, 0.5+\\delta]$, where $\\delta$ is a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_perturbation(data, probability):\n",
    "    diff = abs(probability - 0.5)\n",
    "    k = 2 * diff ## factor 2 is a hyperparameter\n",
    "    noise = torch.randn_like(data) * torch.sqrt(torch.tensor(k))\n",
    "    perturbed_data = data + noise\n",
    "    return perturbed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLossLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLossLinear, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, net, x, y, v):\n",
    "        x.requires_grad_(True)  # Ensure x requires gradients\n",
    "\n",
    "        # Compute the output and the BCE loss\n",
    "        out = net(x).squeeze()\n",
    "        bce_loss = self.bce_loss(out, y)\n",
    "\n",
    "        # Zero the gradients of x\n",
    "        if x.grad is not None:\n",
    "            x.grad.zero_()\n",
    "\n",
    "        # Compute the gradient of the output with respect to the input\n",
    "        out.backward(torch.ones_like(out), retain_graph=True)\n",
    "        grad = x.grad\n",
    "        # Normalize the gradient and v\n",
    "        grad_norm = grad / grad.norm()\n",
    "        v = v / v.norm()\n",
    "\n",
    "        # Compute the inner product of the normalized gradient and v\n",
    "        inner_product = torch.abs(torch.dot(grad_norm.view(-1), v.view(-1)))\n",
    "\n",
    "        # The final loss is the sum of the BCE loss and the inner product\n",
    "        factor = 0\n",
    "        if out < 0.51 and out > 0.49:\n",
    "            factor = 1\n",
    "        loss = bce_loss + factor * inner_product\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class CustomLossExponential(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLossExponential, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, net, x, y, v):\n",
    "        # Compute the output and the BCE loss\n",
    "        out = net(x).squeeze()\n",
    "        bce_loss = self.bce_loss(out, y)\n",
    "\n",
    "        # Compute the gradient of the output with respect to the input\n",
    "        out.backward(torch.ones_like(out), retain_graph=True)\n",
    "        grad = x.grad\n",
    "        # Normalize the gradient\n",
    "        grad_norm = grad / grad.norm()\n",
    "        v = v / v.norm()\n",
    "\n",
    "        # Compute the inner product of the normalized gradient and v\n",
    "        inner_product = torch.abs(torch.dot(grad_norm.view(-1), v.view(-1)))\n",
    "\n",
    "        # The final loss is the sum of the BCE loss and the inner product\n",
    "        factor = 0\n",
    "        if out < 0.51 and out > 0.49:\n",
    "            factor = 1\n",
    "        loss = bce_loss + factor * math.exp(inner_product)\n",
    "\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class CustomLossTangent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLossTangent, self).__init__()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, net, x, y, v):\n",
    "        # Compute the output and the BCE loss\n",
    "        out = net(x).squeeze()\n",
    "        bce_loss = self.bce_loss(out, y)\n",
    "\n",
    "        # Compute the gradient of the output with respect to the input\n",
    "        out.backward(torch.ones_like(out), retain_graph=True)\n",
    "        grad = x.grad\n",
    "        # Normalize the gradient\n",
    "        grad_norm = grad / grad.norm()\n",
    "        v = v / v.norm()\n",
    "\n",
    "        # Compute the inner product of the normalized gradient and v\n",
    "        inner_product = torch.abs(torch.dot(grad_norm.view(-1), v.view(-1)))\n",
    "\n",
    "        # The final loss is the sum of the BCE loss and the inner product\n",
    "        factor = 0\n",
    "        if out < 0.51 and out > 0.49:\n",
    "            factor = 1\n",
    "        loss = bce_loss + factor * math.tan(inner_product* math.pi / 2)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start to train the model in the following scheme: For each epoch, we first iterate over all data in the current dataset to optimize the model. After each epoch is done, each point in the training set should receive a probability between 0 and 1, and we take out those points with probability in the range $[0.25, 0.75]$, and apply the perturbation method to generate new datapoints and add to the dataset, and we start the next epoch. We repeat the training process with each of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_linear = CustomLossLinear()\n",
    "criterion_exponential = CustomLossExponential()\n",
    "criterion_tangent = CustomLossTangent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size, expected tensor [96] and src [9] to have the same number of elements, but got 96 and 9 elements respectively",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m     22\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([k\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m y], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Backpropagate the loss\u001b[39;00m\n\u001b[0;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\Github\\Estimate-Basin-Boundary\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Github\\Estimate-Basin-Boundary\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m, in \u001b[0;36mCustomLossLinear.forward\u001b[1;34m(self, net, x, y, v)\u001b[0m\n\u001b[0;32m     22\u001b[0m v \u001b[38;5;241m=\u001b[39m v \u001b[38;5;241m/\u001b[39m v\u001b[38;5;241m.\u001b[39mnorm()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Compute the inner product of the normalized gradient and v\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m inner_product \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# The final loss is the sum of the BCE loss and the inner product\u001b[39;00m\n\u001b[0;32m     28\u001b[0m factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: inconsistent tensor size, expected tensor [96] and src [9] to have the same number of elements, but got 96 and 9 elements respectively"
     ]
    }
   ],
   "source": [
    "# Initialize the loss and the optimizer\n",
    "loss_fn = criterion_linear\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  \n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(10): \n",
    "    running_loss = 0.0\n",
    "    for x, y in dataloader_train: \n",
    "        # Compute v by running the vector over lorenz\n",
    "        v = lorenz_data(x)  # replace lorenz with your lorenz function\n",
    "\n",
    "        # Convert v to a tensor if it's not already one\n",
    "        if not isinstance(v, torch.Tensor):\n",
    "            v = [item for sublist in v for item in sublist]  # Flatten the list\n",
    "            v = [float(i) for i in v]\n",
    "            v = torch.tensor(v)\n",
    "\n",
    "        # Zero the gradients\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # Compute the loss\n",
    "        y = torch.tensor([k==1 for k in y], dtype=torch.float32)\n",
    "        loss = loss_fn(net, x, y, v)\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add the loss of this batch to the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print average loss per epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(dataloader_train)}\")\n",
    "\n",
    "    # After each epoch, augment the dataset\n",
    "    with torch.no_grad():\n",
    "        # Get the predictions for the entire dataset\n",
    "        predictions = net(dataset_train)  # replace x_all with your entire dataset\n",
    "\n",
    "        # Find the indices of the data points with predicted probability within the range [0.25, 0.75]\n",
    "        indices = ((predictions >= 0.25) & (predictions <= 0.75)).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # Take out these data points\n",
    "        x_to_augment = dataset_train[indices]\n",
    "\n",
    "        # Apply data_perturbation to each of them to generate a new set of data points\n",
    "        x_augmented = data_perturbation(x_to_augment)  # replace data_perturbation with your data perturbation function\n",
    "        # Apply simulation to each point in x_augmented\n",
    "        simulation_results = [simulation(x[0], x[1], x[2]) for x in x_augmented]\n",
    "\n",
    "        # Convert the boolean results to integers (1 for True, -1 for False)\n",
    "        y_augmented = [int(result)*2 - 1 for result in simulation_results]\n",
    "\n",
    "        # Convert the list to a PyTorch tensor\n",
    "        y_augmented = torch.tensor(y_augmented)\n",
    "\n",
    "\n",
    "        # Add these data points to the original training set to form an enlarged training set\n",
    "        x_all = torch.cat([x_all, x_augmented])\n",
    "        y_all = torch.cat([y_all, y_augmented])\n",
    "\n",
    "        # Create a new DataLoader for the enlarged training set\n",
    "        dataset = TensorDataset(x_all, y_all)\n",
    "        dataloader_train = DataLoader(dataset, batch_size=32)  # replace 32 with your batch size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
